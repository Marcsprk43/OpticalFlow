{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner detection and 3D calibration\n",
    "Based on the tutorial [here](https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html)\n",
    "\n",
    "This is an improvement on the previous notebook. In this notebook, the stereo images are processed as pairs.   This is important because during the corner mapping, only the corners that appear in both images must be saved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'gerrie'\n",
    "#user = 'marcvanzyl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, os\n",
    "from cv2 import aruco\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The board\n",
    "The following line of the board was updated to reflect the correct scale of the board.  This is necessary becuase the `board` object is used later   \n",
    "`board = aruco.CharucoBoard_create(7, 5, .04026, .8*.04026, aruco_dict)`\n",
    "\n",
    "Notice the board object returned.  Is contains all the magic of the CharUco pattern. For the image interpretation and to calibrate the cameras the cameras need a picture of the board and also information about the picture (ie. the details of the board).  This is all contained in the board object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size = 'experimental setup'\n",
    "\n",
    "if board_size == '7x5':\n",
    "    chessboard_num_squares_across = 7\n",
    "    chessboard_num_squares_up = 5\n",
    "    chessboard_square_size = 0.04026\n",
    "    chessboard_aruco_ratio = 0.8   # this is a fraction of chessboard_square_size\n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)\n",
    "\n",
    "\n",
    "if board_size == '12x8':\n",
    "    chessboard_num_squares_across = 12\n",
    "    chessboard_num_squares_up = 8\n",
    "    chessboard_square_size = 1\n",
    "    chessboard_aruco_ratio = 0.7   # this is a fraction of chessboard_square_size\n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_5X5_250)\n",
    "\n",
    "elif board_size == 'experimental setup':   \n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_4X4_50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board object contains all the vectors (pointing from the bottom left corner) to each of the corners present on the board.  The two kinds of objects are **aruco markers** and the **checkerboard**.  The aruco markers are called '`markers`' in the code and documentation.  The term `marker corners` means the set of 4 corners around each aruco maker. \n",
    "\n",
    "The aruco markers can be extracted from the board using the `aruco.getBoardObjectAndImagePoints(board, makerCorners command)`. \n",
    "\n",
    "Once the algorithm detected the markerCorners then it can interpolate between the marker corners to find the checkerboard corners.  The positions of checkerboard \"inside\" corners can be extracted using the folowing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corners are labeled starting from 0 (bottom left) and going right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using Charuco\n",
    "\n",
    "This function:\n",
    "1. finds the locations of the corners of the aruco squares (`cv2.aruco.detectMarkers`)\n",
    "1. if markers were found interpolates to find the checkerboard markers between them (`cv2.aruco.interpolateCornersCharuco`)\n",
    "1. zooms into each checkerboard corner to get sub-pixel accuracy using (`cv2.cornerSubPix`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkerboard_corners(img, board, clipLimit=2.0, verbose=False):\n",
    "\n",
    "    # These are parameters used by the cv2.cornerSubPix function\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.00001)\n",
    "\n",
    "    # increase the contraxt\n",
    "    img = increase_contrast(img, clipLimit=clipLimit)\n",
    " \n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    detect_params = aruco.CORNER_REFINE_SUBPIX\n",
    "\n",
    "    # find the aruco corners and the ids of each corner\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict, detect_params)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Found {} aruco marker corners'.format(len(ids)))\n",
    "    \n",
    "    if len(ids)>0:\n",
    "        (retval, charucoCorners,\n",
    "         charucoIds) = cv2.aruco.interpolateCornersCharuco(corners, ids, gray, board, )\n",
    "        if verbose:\n",
    "            print('Found {} checker corners'.format(len(charucoIds)))\n",
    "        if len(charucoIds)>0:\n",
    "            # SUB PIXEL DETECTION\n",
    "            for corner in charucoCorners:\n",
    "                if verbose:\n",
    "                    print('Sub pixel optimization:')\n",
    "                    print(corner)\n",
    "                cv2.cornerSubPix(gray, corner,\n",
    "                                 winSize = (5,5),\n",
    "                                 zeroZone = (-1,-1),\n",
    "                                 criteria = criteria)\n",
    "                if verbose:\n",
    "                    print(corner)\n",
    "                    print('+++')\n",
    "        \n",
    "    return charucoCorners, charucoIds, gray.shape\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big problem is that you need to give the stereo calibration only coners that appear in both cameras.  Fortunately, the detection returns the ids of the corners in the `charucoIds`. These `charucoIds` correspond to the numbering system mentioned above (bottom left is 0 and starts going across to the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_corners(cornersL, idsL, cornersR, idsR, verbose=False):\n",
    "    # find the list of corners common to both images\n",
    "    # get a list of the ids\n",
    "    listIdsL = idsL.reshape((1,-1))[0].tolist()\n",
    "    listIdsR = idsR.reshape((1,-1))[0].tolist()\n",
    "    \n",
    "    # this selects only the points that are in listIdsA and listIdsC\n",
    "    commonIds = list(set(listIdsL).intersection(set(listIdsR)))\n",
    "\n",
    "    if verbose:\n",
    "        print('Found {} common IDS between L:({}) and R:({})'.format(len(commonIds),\n",
    "                                                                     len(idsL),\n",
    "                                                                     len(idsR)))\n",
    "\n",
    "    # Reindex L camera data by taking out only the common points \n",
    "    re_index = [ listIdsL.index(ind) for ind in commonIds]\n",
    "    idsL_new = idsL[re_index]\n",
    "    cornersL_new = np.array(cornersL)[re_index]\n",
    "    cornersL_new = [x for x in cornersL_new]\n",
    "\n",
    "    # Reindex R camera data by taking out only the common points\n",
    "    re_index = [ listIdsR.index(ind) for ind in commonIds]\n",
    "    idsR_new = idsR[re_index]\n",
    "    cornersR_new = np.array(cornersR)[re_index]\n",
    "    cornersR_new = [x for x in cornersR_new]\n",
    "\n",
    "    return np.array(cornersL_new), idsL_new, np.array(cornersR_new), idsR_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cam_calOF = pickle.load( open('OFCameraCalibration.p', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_calOF.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera features:\n",
    "- sensor size = 3.68 x 2.76 mm  \n",
    "- sensor resolution  = 3280 Ã— 2464\n",
    "- focal length = 3.04 mm\n",
    "\n",
    "$$ d_{mm} = \\frac{pix \\times 3.68}{3280} $$\n",
    "\n",
    "The depth can now be found\n",
    "$$ Z = \\frac{T \\times f}{d_{mm}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from here https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv\n",
    "def increase_contrast(img, clipLimit=3.0, verbose=False):\n",
    "\n",
    "    if clipLimit>0.0:\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        if verbose:\n",
    "            cv2.imshow(\"lab\",lab)\n",
    "\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        if verbose:\n",
    "            cv2.imshow('l_channel', l)\n",
    "            cv2.imshow('a_channel', a)\n",
    "            cv2.imshow('b_channel', b)\n",
    "\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        if verbose:\n",
    "            cv2.imshow('CLAHE output', cl)\n",
    "\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        if verbose:\n",
    "            cv2.imshow('limg', limg)\n",
    "\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        if verbose:\n",
    "            cv2.imshow('final', final)\n",
    "    else:\n",
    "        final = img.copy()\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_charuco_marker_corners(img, aruco_dict, clipLimit=3.0, verbose=False):\n",
    "\n",
    "    # These are parameters used by the cv2.cornerSubPix function\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10000, 1e-11)\n",
    "\n",
    "    # increase the contraxt\n",
    "    img = increase_contrast(img, clipLimit=clipLimit)\n",
    "\n",
    "\n",
    " \n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    # find the aruco corners and the ids of each corner\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict)\n",
    "\n",
    "    if verbose:\n",
    "        print('Found {} aruco marker corners'.format(len(ids)))\n",
    "        \n",
    "    \n",
    "    #  now find the central points of the aruco markers by averaging the four corner points\n",
    "    if  len(ids) > 0:\n",
    "        # SUB PIXEL DETECTION\n",
    "        for corner in corners:\n",
    "            if verbose:\n",
    "                print('Sub pixel optimization:')\n",
    "                print(corner)\n",
    "            cv2.cornerSubPix(gray, corner,\n",
    "                             winSize = (4,4),\n",
    "                             zeroZone = (-1,-1),\n",
    "                             criteria = criteria)\n",
    "            if verbose:\n",
    "                print(corner)\n",
    "                print('+++')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(corners), ids, gray.shape\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_aruco_center(corners, ids):\n",
    "    # find the center point of the 4 corner of the aruco markers\n",
    "    centers = []\n",
    "    for cnrs in corners:\n",
    "        center = np.array((np.average(cnrs[0][:,0]), np.average(cnrs[0][:,1])))\n",
    "        centers.append(center)\n",
    "    centers = np.array(centers)\n",
    "    centers = np.array(centers).reshape((-1,1,2))\n",
    "    center_ids = np.arange(centers.shape[0]).reshape(-1,1)\n",
    "    \n",
    "    return centers, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/Users/{}/Google Drive/ScienceFair2021/Random_video/500/\"\n",
    "datadir = \"/Users/{}/Google Drive/ScienceFair2021/DataCapture/realData/\".format(user)\n",
    "\n",
    "video_files = np.array([f for f in os.listdir(datadir) if f.endswith(\".mp4\") ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture('{}{}'.format(datadir, 'Rotate_detailed_0_-5-3-00908I.mp4'))\n",
    "#video_capture.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'h264'))\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))\n",
    "\n",
    "#cap.set(CV_CAP_PROP_FOURCC, CV_FOURCC('H', '2', '6', '4'));\n",
    "#video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "#video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "frame = 0\n",
    "while video_capture.isOpened():\n",
    "    ret, image = video_capture.read()\n",
    "    \n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    if frame%20 == 0:\n",
    "        image2 = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        plt.imshow(image2,cmap='gray')\n",
    "        plt.title('Frame {}'.format(frame))\n",
    "        plt.show()\n",
    "        plt.imshow(cv2.cvtColor(increase_contrast(image, clipLimit=3.0, verbose=False), cv2.COLOR_BGR2GRAY),cmap='gray')\n",
    "        plt.title('Frame {}'.format(frame))\n",
    "        plt.show()\n",
    "    frame += 1\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata.blocks import block_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set verbose = True to make plots of each frame\n",
    "verbose =  True\n",
    "\n",
    "for file in [video_files[3]]:\n",
    "    \n",
    "    result_list = []\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # the size of the square to average the flow over\n",
    "    center_range = 10\n",
    "\n",
    "    cap = cv2.VideoCapture(datadir + file)\n",
    "    print(\"Working with: {}\".format(file))\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # discard the first 20 frames becuase the movment may be accelerating\n",
    "    for i in range(20):\n",
    "        ret, curr_pic_color = cap.read()\n",
    "\n",
    "    ret, frame1 = cap.read()\n",
    "    last_pic_3_color = cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "    last_pic_3_grey = cv2.cvtColor(last_pic_3_color,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    ret, frame1 = cap.read()\n",
    "    last_pic_2_color = cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "    last_pic_2_grey = cv2.cvtColor(last_pic_2_color,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    ret, frame1 = cap.read()\n",
    "    last_pic_1_color = cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "    last_pic_grey = cv2.cvtColor(last_pic_1_color,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    last_pic_grey = last_pic_grey[:,200:-200]\n",
    "\n",
    "    \n",
    "\n",
    "    hsv = np.zeros_like(frame1[:,200:-200,:])\n",
    "    hsv[...,1] = 255\n",
    "    flow = np.zeros_like(frame1[:,200:-200,:])\n",
    "\n",
    "    # this creates the array for the sorted centers \n",
    "    sorted_centers = np.zeros([10,2])\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "\n",
    "        ret, curr_pic_color = cap.read()\n",
    "        ret, curr_pic_color = cap.read()\n",
    "        ret, curr_pic_color = cap.read()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ret will be False if there are no more frames\n",
    "        if ret == False:\n",
    "            break\n",
    "            \n",
    "        counter += 1\n",
    "        if counter > 120:\n",
    "            break\n",
    "        curr_pic_color = cv2.undistort(curr_pic_color,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "        curr_pic_grey = cv2.cvtColor(curr_pic_color, cv2.COLOR_BGR2GRAY)\n",
    "        curr_pic_grey = curr_pic_grey[:,200:-200]\n",
    "\n",
    "        if counter > 1:\n",
    "            flags = cv2.OPTFLOW_USE_INITIAL_FLOW\n",
    "        else:\n",
    "            flags = 0\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(last_pic_grey, curr_pic_grey, flow, 0.5, 3, 50, 6, 7, 1.2, flags)\n",
    "\n",
    "        # clip flows to clean up the data\n",
    "        flow[flow>100] = 0\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Flow: mean: {} max: {}  min: {} {}\".format(flow.mean(), flow.max(), flow.min(), flow.shape))\n",
    "\n",
    "\n",
    "            mag = (cv2.multiply(cv2.add(flow[...,0],0.),6.))\n",
    "\n",
    "            mag[mag<0.0] = 0\n",
    "            mag[mag>255] = 255\n",
    "\n",
    "            print(\"Mag: mean: {} max: {}  min: {} {}\".format(mag.mean(), mag.max(), mag.min(), flow.shape))\n",
    "\n",
    "            # edge detection for the outlines\n",
    "            edges = cv2.Canny(curr_pic_grey, 50, 100)\n",
    "\n",
    "\n",
    "            hsv[...,0] = 0\n",
    "            #hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "            hsv[...,2] = mag\n",
    "            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "\n",
    "            annot_font = {'fontname':'Arial', 'size':'14','weight':'bold'}\n",
    "\n",
    "            fig, axs = plt.subplots(figsize=(36,24))\n",
    "            rgb[edges>100,1] = 255 \n",
    "            axs.imshow(rgb)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            flow_x = flow[:,:,0]\n",
    "            flow_y = flow[:,:,1]\n",
    "\n",
    "            flow_x_s = block_reduce(flow_x.reshape((2464, 2864)), 64, func=np.mean).T\n",
    "            flow_y_s = block_reduce(flow_y.reshape((2464, 2864)), 64, func=np.mean).T\n",
    "\n",
    "\n",
    "            x_ticks = flow_x_s.shape[0]\n",
    "            y_ticks = flow_x_s.shape[1]\n",
    "\n",
    "\n",
    "            X, Y = np.mgrid[0:x_ticks, 0:y_ticks]\n",
    "\n",
    "            fig, axs = plt.subplots(figsize=(36,24))\n",
    "            axs.quiver(X, Y, flow_x_s*4, flow_y_s*4, alpha=.5)\n",
    "\n",
    "\n",
    "            plt.show() \n",
    "\n",
    "        last_pic_grey = np.copy(curr_pic_grey)\n",
    "        \n",
    "    pickle.dump(np.array(result_list)[5:105], open('{}{}_res.p'.format(datadir, file[:-5]), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = int(flow.shape[1]/2-200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_slice = flow[:,edge:-edge,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_mean = np.mean(flow_slice[:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_std = np.std(flow_slice[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_slice_clean = flow_slice[:,:,0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_slice_clean[flow_slice_clean>slice_mean+2*slice_std] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set verbose = True to make plots of each frame\n",
    "verbose =  True\n",
    "\n",
    "for file in [video_files[3]]:\n",
    "    \n",
    "    result_list = []\n",
    "\n",
    "    counter = 0\n",
    "    #2175\n",
    "    top = []\n",
    "    #1087\n",
    "    middle = []\n",
    "    #0\n",
    "    bottom = []\n",
    "    # the size of the square to average the flow over\n",
    "    center_range = 10\n",
    "\n",
    "    cap = cv2.VideoCapture(datadir + file)\n",
    "    print(\"Working with: {}\".format(file))\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # discard the first 20 frames becuase the movment may be accelerating\n",
    "    for i in range(20):\n",
    "        ret, curr_pic_color = cap.read()\n",
    "\n",
    "    ret, frame1 = cap.read()\n",
    "    last_pic_3_color = cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "    last_pic_3_grey = cv2.cvtColor(last_pic_3_color,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    ret, frame1 = cap.read()\n",
    "    last_pic_2_color = cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "    last_pic_2_grey = cv2.cvtColor(last_pic_2_color,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    ret, frame1 = cap.read()\n",
    "    last_pic_1_color = cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "    last_pic_grey = cv2.cvtColor(last_pic_1_color,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[...,1] = 255\n",
    "    flow = np.zeros_like(frame1)\n",
    "\n",
    "    # this creates the array for the sorted centers \n",
    "    sorted_centers = np.zeros([10,2])\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "\n",
    "        ret, curr_pic_color = cap.read()\n",
    "        ret, curr_pic_color = cap.read()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ret will be False if there are no more frames\n",
    "        if ret == False:\n",
    "            break\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "        threshold = int((3/8)*length)\n",
    "        mid = length/2\n",
    "        if mid+threshold < counter:\n",
    "            break\n",
    "        elif counter == mid-threshold-1:\n",
    "            ret, frame1 = cap.read()\n",
    "            last_pic_1_color = cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "            last_pic_grey = cv2.cvtColor(last_pic_1_color,cv2.COLOR_BGR2GRAY)\n",
    "        elif counter > mid-threshold:\n",
    "            print(counter)\n",
    "            ret, curr_pic_color = cap.read()\n",
    "            curr_pic_color = cv2.undistort(curr_pic_color,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "            curr_pic_grey = cv2.cvtColor(curr_pic_color, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            if counter > 1:\n",
    "                flags = cv2.OPTFLOW_USE_INITIAL_FLOW\n",
    "            else:\n",
    "                flags = 0\n",
    "\n",
    "            flow = cv2.calcOpticalFlowFarneback(last_pic_grey, curr_pic_grey, flow, 0.5, 3, 50, 6, 7, 1.2, flags)\n",
    "\n",
    "            # clip flows to clean up the data\n",
    "            mean = np.mean(flow)\n",
    "            print(\"Mean of frame: {}\".format(mean))\n",
    "            flow[flow>100] = 0\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Flow: mean: {} max: {}  min: {} {}\".format(flow.mean(), flow.max(), flow.min(), flow.shape))\n",
    "\n",
    "\n",
    "                mag = (cv2.multiply(cv2.add(flow[...,0],0.),6.))\n",
    "\n",
    "                mag[mag<0.0] = 0\n",
    "                mag[mag>255] = 255\n",
    "\n",
    "                print(\"Mag: mean: {} max: {}  min: {} {}\".format(mag.mean(), mag.max(), mag.min(), flow.shape))\n",
    "\n",
    "                # edge detection for the outlines\n",
    "                edges = cv2.Canny(curr_pic_grey, 50, 100)\n",
    "\n",
    "\n",
    "                hsv[...,0] = 0\n",
    "                #hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "                hsv[...,2] = mag\n",
    "                rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "\n",
    "                annot_font = {'fontname':'Arial', 'size':'14','weight':'bold'}\n",
    "\n",
    "                fig, axs = plt.subplots(figsize=(36,24))\n",
    "                rgb[edges>100,1] = 255 \n",
    "                axs.imshow(rgb)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "                flow_x = flow[:,:,0]\n",
    "                flow_y = flow[:,:,1]\n",
    "\n",
    "                flow_x_s = block_reduce(flow_x.reshape((2464, 3264)), 32, func=np.mean).T\n",
    "                flow_y_s = block_reduce(flow_y.reshape((2464, 3264)), 32, func=np.mean).T\n",
    "\n",
    "\n",
    "                x_ticks = flow_x_s.shape[0]\n",
    "                y_ticks = flow_x_s.shape[1]\n",
    "\n",
    "\n",
    "                X, Y = np.mgrid[0:x_ticks, 0:y_ticks]\n",
    "\n",
    "                fig, axs = plt.subplots(figsize=(36,24))\n",
    "                axs.quiver(X, Y, flow_x_s, flow_y_s, alpha=.5)\n",
    "\n",
    "\n",
    "                plt.show() \n",
    "\n",
    "            last_pic_grey = np.copy(curr_pic_grey)\n",
    "\n",
    "    pickle.dump(np.array(result_list)[5:105], open('{}{}_res.p'.format(datadir, file[:-5]), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata.utils import block_reduce\n",
    "flow_x = flow[:,:,0]\n",
    "flow_y = flow[:,:,1]\n",
    "\n",
    "flow_x_s = block_reduce(flow_y.reshape((2464, 3264)), 32, func=np.mean)\n",
    "flow_y_s = block_reduce(flow_y.reshape((2464, 3264)), 32, func=np.mean)\n",
    "\n",
    "\n",
    "x_ticks = flow_x_s.shape[0]\n",
    "y_ticks = flow_x_s.shape[1]\n",
    "\n",
    "\n",
    "X, Y = np.mgrid[0:x_ticks, 0:y_ticks]\n",
    "\n",
    "\n",
    "plt.quiver(X, Y, flow_x_s, flow_y_s, scale=1, alpha=.5)\n",
    "\n",
    "plt.xlim(-1, n)\n",
    "plt.xticks([])\n",
    "plt.ylim(-1, n)\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_x_ss = block_reduce(flow_x_s.reshape((1232, 1632)), 2, func=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_x_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1inds = center_ids.argsort()\n",
    "print(arr1inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centers = centers[arr1inds]\n",
    "sorted_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_ids[arr1inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(np.array(result_list), open('{}{}_res.p'.format(datadir, file[:-5]), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{}{}_res.p'.format(datadir, file[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result_list)[5:105].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow[...,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow[...,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerx = 100\n",
    "centery = 120\n",
    "\n",
    "flow[centery-10:centery+10,centerx-10:centerx+10][:,:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow[centery-10:centery+10,centerx-10:centerx+10][:,:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(center_idsL.reshape(-1)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centersL[center_idsL.reshape(-1)[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centers = np.zeros([10,2])\n",
    "sorted_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in center_idsL[1:]:\n",
    "    sorted_centers[i] = centersL[1:][counter]\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centersL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_idsL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Frame', 'ArucoId', 'XPos', 'YPos', 'Pos', 'File'])\n",
    "\n",
    "for i, points in enumerate(centersAll):\n",
    "    for j, point in enumerate(points): \n",
    "        ser = pd.Series()\n",
    "        ser['File'] = \"images[i]\"\n",
    "        ser['Frame'] = i\n",
    "        ser['ArucoId'] = centersIdsAll[i][j][0]\n",
    "        ser['Pos'] = point[0]\n",
    "        ser['XPos'] = point[0][0]\n",
    "        ser['YPos'] = point[0][1]\n",
    "\n",
    "        #results = results.append(ser,ignore_index=True)\n",
    "        results.loc[i*10+j] = ser\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_003 = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results_003, open('results_003.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_distance = np.array([1169.66362686, 1405.20212069, 1654.65192714, 1912.57758013,\n",
    "       2175.9671413 , 2443.05403133, 2531.44557732, 2531.73156376,\n",
    "       2136.68322387, 2135.49530535])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera features:\n",
    "- sensor size = 3.68 x 2.76 mm  \n",
    "- sensor resolution  = 3280 Ã— 2464\n",
    "- focal length = 3.04 mm\n",
    "\n",
    "$$ d_{mm} = \\frac{pix \\times 3.68}{3280} $$\n",
    "\n",
    "The depth can now be found\n",
    "$$ Z = \\frac{T \\times f}{d_{mm}} $$\n",
    "\n",
    "In the case of optical flow $T$ is the distance travelled between frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_focal_length = 3.04\n",
    "sensor_size_x = 3.68\n",
    "sensor_resolution_x = 3280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_distance[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = results.groupby('ArucoId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[results['ArucoId']<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = pd.MultiIndex.from_tuples([('15mm/s', 'Distance'),('15mm/s', 'Error'),('15mm/s', 'Std Dev')], names=['Camera Speed', 'Metric'])\n",
    "final =  pd.DataFrame(columns = col_index)\n",
    "\n",
    "\n",
    "for step in range(1,6):\n",
    "    # create the heading for the results dataframe\n",
    "    speed_str = '{}mm/s'.format(step*15)\n",
    "    # this is the distance the camera traveled each frame\n",
    "    camera_distance = step\n",
    "    \n",
    "    step_results = results[results['Frame']%step == 0].copy()\n",
    "    step_grp = step_results.groupby('ArucoId')\n",
    "    for marker_id in step_grp.groups.keys():\n",
    "        aruco_id = step_grp.get_group(marker_id)\n",
    "        aruco_id['DisparityPixel'] = aruco_id['XPos'].diff()\n",
    "        aruco_id['CameraSteps'] = aruco_id['Frame'].diff()/step    \n",
    "        print(aruco_id.shape)\n",
    "\n",
    "        aruco_id['DisparityPixelPerStep'] = aruco_id['DisparityPixel']\n",
    "        aruco_id['DisparityMM'] = aruco_id['DisparityPixelPerStep']*sensor_size_x/sensor_resolution_x\n",
    "        distance = camera_distance*camera_focal_length/aruco_id['DisparityMM'].mean()\n",
    "        std_dev = camera_focal_length/aruco_id['DisparityMM'].std()/camera_distance\n",
    "        error = (camera_distance*camera_focal_length/aruco_id['DisparityMM']) - actual_distance[marker_id]\n",
    "        mean_error = error[abs(error)<1000 ].mean(skipna=True)\n",
    "        std_dev_error = error[abs(error)<1000 ].std()\n",
    "\n",
    "        final.loc[marker_id, (speed_str, 'Distance')] = round(distance,1)\n",
    "        final.loc[marker_id, (speed_str, 'Error')] = round(mean_error,1)\n",
    "        final.loc[marker_id, (speed_str, 'Std Dev')] = round(std_dev_error, 1)\n",
    "        final.loc[marker_id, (speed_str, 'Max')] =  round(error[abs(error)<1000 ].max(), 1)\n",
    "        final.loc[marker_id, (speed_str, 'Min')] = round(error[abs(error)<1000 ].min(),1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aruco_id['DisparityPixelPerStep'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.index.name = 'Target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_003 = final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_003, open('of_final_003.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
