{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner detection and 3D calibration\n",
    "Based on the tutorial [here](https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html)\n",
    "\n",
    "This is an improvement on the previous notebook. In this notebook, the stereo images are processed as pairs.   This is important because during the corner mapping, only the corners that appear in both images must be saved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_name = 'gerrie'\n",
    "#computer_name = 'marcvanzyl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL, os\n",
    "import cv2 as cv\n",
    "from cv2 import aruco\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The board\n",
    "The following line of the board was updated to reflect the correct scale of the board.  This is necessary becuase the `board` object is used later   \n",
    "`board = aruco.CharucoBoard_create(7, 5, .04026, .8*.04026, aruco_dict)`\n",
    "\n",
    "Notice the board object returned.  Is contains all the magic of the CharUco pattern. For the image interpretation and to calibrate the cameras the cameras need a picture of the board and also information about the picture (ie. the details of the board).  This is all contained in the board object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size = '12x8'\n",
    "if board_size == '7x5':\n",
    "    chessboard_num_squares_across = 7\n",
    "    chessboard_num_squares_up = 5\n",
    "    chessboard_square_size = 0.04026\n",
    "    chessboard_aruco_ratio = 0.8   # this is a fraction of chessboard_square_size\n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)\n",
    "\n",
    "\n",
    "elif board_size == '12x8':\n",
    "    chessboard_num_squares_across = 12\n",
    "    chessboard_num_squares_up = 8\n",
    "    chessboard_square_size = 1\n",
    "    chessboard_aruco_ratio = 0.7   # this is a fraction of chessboard_square_size\n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_5X5_250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"/Users/{}/Downloads/\".format(computer_name)\n",
    "\n",
    "board = aruco.CharucoBoard_create(chessboard_num_squares_across, \n",
    "                                  chessboard_num_squares_up, \n",
    "                                  chessboard_square_size, \n",
    "                                  chessboard_aruco_ratio*chessboard_square_size, \n",
    "                                  aruco_dict)\n",
    "\n",
    "imboard = board.draw((12000, 8000))\n",
    "cv.imwrite(workdir + \"chessboard.tiff\", imboard)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.imshow(imboard, cmap = mpl.cm.gray, interpolation = \"nearest\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board object contains all the vectors (pointing from the bottom left corner) to each of the corners present on the board.  The two kinds of objects are **aruco markers** and the **checkerboard**.  The aruco markers are called '`markers`' in the code and documentation.  The term `marker corners` means the set of 4 corners around each aruco maker. \n",
    "\n",
    "The aruco markers can be extracted from the board using the `aruco.getBoardObjectAndImagePoints(board, makerCorners command)`. \n",
    "\n",
    "Once the algorithm detected the markerCorners then it can interpolate between the marker corners to find the checkerboard corners.  The positions of checkerboard \"inside\" corners can be extracted using the folowing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corners are labeled starting from 0 (bottom left) and going right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After taking many pictures of the board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/Users/{}/Google Drive/ScienceFair2021/Calibration/OFJet/\".format(computer_name)\n",
    "\n",
    "images = np.array([datadir + f for f in os.listdir(datadir) if f.endswith(\".jpeg\") ])\n",
    "\n",
    "image_sort_list = []\n",
    "\n",
    "for image in images:\n",
    "    image_sort_list.append(int(image.split('OF-')[1].split('.')[0]))\n",
    "    \n",
    "images = images[np.argsort(image_sort_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imR = cv.imread(datadir+'OF-1.jpeg')\n",
    "\n",
    "fig, axs = plt.subplots(1,1, figsize=(10,18))\n",
    "\n",
    "axs.imshow(imR)\n",
    "axs.set_title(\"RGB\".format(chan))\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,20))\n",
    "\n",
    "for chan in range(3):\n",
    "    \n",
    "\n",
    "    axs[chan].imshow(imR[:,:,chan],cmap='gray')\n",
    "    \n",
    "    axs[chan].set_title(\"color{}\".format(chan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the files and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find corners on one image\n",
    "This is just a test using Harris corner dectector to learn how functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = datadir+'OF-1.jpeg'\n",
    "\n",
    "img = cv.imread(filename)\n",
    "gray_int = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray_int)\n",
    "\n",
    "dst = cv.cornerHarris(gray,2,3,0.05)\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv.dilate(dst,None)\n",
    "dst = cv.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[255,0,0]\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(18,22))\n",
    "axs[0].imshow(dst)\n",
    "axs[0].set_title(\"Plot of the Harris corner detector matrix\")\n",
    "axs[1].imshow(dst>0.01*dst.max())\n",
    "axs[1].set_title(\"Plot of all likely corners dst>0.01*dst.max()\")\n",
    "axs[2].imshow(img)\n",
    "axs[2].set_title(\"img with the detected corners in red\")\n",
    "\n",
    "\n",
    "\n",
    "#ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "corners, ids, rejectedImgPoints = cv.aruco.detectMarkers(gray_int, aruco_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the images was that the contrast was not very high.  See above.  To improve the contrast I found this code online.  clipLimit is a variable that increases the contras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from here https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv\n",
    "def increase_contrast(img, clipLimit=3.0, verbose=False):\n",
    "\n",
    "    if clipLimit>0.0:\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv.cvtColor(img, cv.COLOR_BGR2LAB)\n",
    "        if verbose:\n",
    "            cv.imshow(\"lab\",lab)\n",
    "\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv.split(lab)\n",
    "        if verbose:\n",
    "            cv.imshow('l_channel', l)\n",
    "            cv.imshow('a_channel', a)\n",
    "            cv.imshow('b_channel', b)\n",
    "\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv.createCLAHE(clipLimit=clipLimit, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        if verbose:\n",
    "            cv.imshow('CLAHE output', cl)\n",
    "\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv.merge((cl,a,b))\n",
    "        if verbose:\n",
    "            cv.imshow('limg', limg)\n",
    "\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv.cvtColor(limg, cv.COLOR_LAB2BGR)\n",
    "        if verbose:\n",
    "            cv.imshow('final', final)\n",
    "    else:\n",
    "        final = img.copy()\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = datadir+'OF-1.jpeg'\n",
    "\n",
    "img = increase_contrast(cv.imread(filename), clipLimit=3.0, verbose=False)\n",
    "gray_int = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray_int)\n",
    "\n",
    "dst = cv.cornerHarris(gray,2,3,0.05)\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv.dilate(dst,None)\n",
    "dst = cv.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img_annotated = img.copy()\n",
    "\n",
    "img_annotated[dst>0.01*dst.max()]=[255,0,0]\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(18,22))\n",
    "axs[0].imshow(dst)\n",
    "axs[0].set_title(\"Plot of the Harris corner detector matrix\")\n",
    "axs[1].imshow(dst>0.01*dst.max())\n",
    "axs[1].set_title(\"Plot of all likely corners dst>0.01*dst.max()\")\n",
    "axs[2].imshow(img_annotated)\n",
    "axs[2].set_title(\"img with the detected corners in red\")\n",
    "\n",
    "\n",
    "\n",
    "#ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "corners, ids, rejectedImgPoints = cv.aruco.detectMarkers(gray_int, aruco_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using Charuco\n",
    "\n",
    "This function:\n",
    "1. finds the locations of the corners of the aruco squares (`cv2.aruco.detectMarkers`)\n",
    "1. if markers were found interpolates to find the checkerboard markers between them (`cv2.aruco.interpolateCornersCharuco`)\n",
    "1. zooms into each checkerboard corner to get sub-pixel accuracy using (`cv2.cornerSubPix`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkerboard_corners(img, board, clipLimit=3.0, verbose=False):\n",
    "\n",
    "    # These are parameters used by the cv2.cornerSubPix function\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 100, 0.00001)\n",
    "\n",
    "    # increase the contraxt\n",
    "    img_h_contrast = increase_contrast(img, clipLimit=clipLimit)\n",
    " \n",
    "    # convert the image to grayscale\n",
    "    gray = cv.cvtColor(img_h_contrast, cv.COLOR_BGR2GRAY)\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(18,22))\n",
    "    axs.imshow(gray, cmap='gray')\n",
    "    \n",
    "    #detect_params = aruco.CORNER_REFINE_SUBPIX\n",
    "    detect_params = None\n",
    "\n",
    "\n",
    "    # find the aruco corners and the ids of each corner\n",
    "    corners, ids, rejectedImgPoints = cv.aruco.detectMarkers(gray, aruco_dict, detect_params)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Found {} aruco marker corners'.format(len(ids)))\n",
    "    \n",
    "    if len(ids)>0:\n",
    "        (retval, charucoCorners,\n",
    "         charucoIds) = cv.aruco.interpolateCornersCharuco(corners, ids, gray, board, )\n",
    "        if verbose:\n",
    "            print('Found {} checker corners'.format(len(charucoIds)))\n",
    "        if len(charucoIds)>0:\n",
    "            # SUB PIXEL DETECTION\n",
    "            for corner in charucoCorners:\n",
    "                if verbose:\n",
    "                    print('Sub pixel optimization:')\n",
    "                    print(corner)\n",
    "                cv.cornerSubPix(gray, corner,\n",
    "                                 winSize = (5,5),\n",
    "                                 zeroZone = (-1,-1),\n",
    "                                 criteria = criteria)\n",
    "                if verbose:\n",
    "                    print(corner)\n",
    "                    print('+++')\n",
    "        \n",
    "    return charucoCorners, charucoIds, gray.shape\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big problem is that you need to give the  calibration only coners that appear in both cameras.  Fortunately, the detection returns the ids of the corners in the `charucoIds`. These `charucoIds` correspond to the numbering system mentioned above (bottom left is 0 and starts going across to the right)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_common_corners(cornersL, idsL, cornersR, idsR, verbose=False):\n",
    "    # find the list of corners common to both images\n",
    "    # get a list of the ids\n",
    "    listIdsL = idsL.reshape((1,-1))[0].tolist()\n",
    "    listIdsR = idsR.reshape((1,-1))[0].tolist()\n",
    "    \n",
    "    # this selects only the points that are in listIdsA and listIdsC\n",
    "    commonIds = list(set(listIdsL).intersection(set(listIdsR)))\n",
    "\n",
    "    if verbose:\n",
    "        print('Found {} common IDS between L:({}) and R:({})'.format(len(commonIds),\n",
    "                                                                     len(idsL),\n",
    "                                                                     len(idsR)))\n",
    "\n",
    "    # Reindex L camera data by taking out only the common points \n",
    "    re_index = [ listIdsL.index(ind) for ind in commonIds]\n",
    "    idsL_new = idsL[re_index]\n",
    "    cornersL_new = np.array(cornersL)[re_index]\n",
    "    cornersL_new = [x for x in cornersL_new]\n",
    "\n",
    "    # Reindex R camera data by taking out only the common points\n",
    "    re_index = [ listIdsR.index(ind) for ind in commonIds]\n",
    "    idsR_new = idsR[re_index]\n",
    "    cornersR_new = np.array(cornersR)[re_index]\n",
    "    cornersR_new = [x for x in cornersR_new]\n",
    "\n",
    "    return np.array(cornersL_new), idsL_new, np.array(cornersR_new), idsR_new\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function just gets the postion of the `object` (ie. the checkerboard) corners in the objects frame and then sorts them according to the pointids list/vector.  This is to aling the board points with the detected image points so that the algorithm can match them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_board_object_points(board, pointids):\n",
    "    corners = board.chessboardCorners\n",
    "    return corners[pointids.reshape((len(pointids)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The real work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up list variables to store the results\n",
    "allIds = []\n",
    "allCorners = []\n",
    "allObjectPoints = []\n",
    "allImagePoints = []\n",
    "\n",
    "# get files by looping over the images array\n",
    "for ind, image in enumerate(images):\n",
    "\n",
    "    filename = image\n",
    "    img = cv.imread(filename)\n",
    "    \n",
    "    print('{}:{}'.format(ind, image))\n",
    "    \n",
    "    # find the corners of the checkerboard between the aruco markers\n",
    "    (corners, \n",
    "     ids, \n",
    "     imsize) = find_checkerboard_corners(img, board, clipLimit=0.0, verbose=False)\n",
    "\n",
    "    # once we have the common ids get the board object points that correspond to the common ids\n",
    "    object_pts = get_board_object_points(board, ids )\n",
    "\n",
    "    # store the results for this image\n",
    "    allCorners.append(corners)\n",
    "    allIds.append(ids)\n",
    "    allObjectPoints.append(object_pts)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,1,figsize=(36,12))\n",
    "    axs.imshow(cv.aruco.drawDetectedCornersCharuco(img, corners, ids))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIP Note:\n",
    "The analysis uses different components in the openCV library.  Some components use image size as (width, height) while others use (height,width).  Here we create two variables:\n",
    "- imsize (height, width)\n",
    "- imsize_t (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the transposed imsize used in the rectify and remap\n",
    "imsize_t = (imsize[1], imsize[0])\n",
    "imsize_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera calibration\n",
    "\n",
    "The stereo claibration works much better if the cameras are calibrated.  We should have done this first, but do it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera(allCorners,allIds, board, imsize):\n",
    "    \"\"\"\n",
    "    Calibrates the camera using the dected corners.\n",
    "    \"\"\"\n",
    "    print(\"CAMERA CALIBRATION\")\n",
    "\n",
    "    cameraMatrixInit = np.array([[ 1000.,    0., imsize[0]/2.],\n",
    "                                 [    0., 1000., imsize[1]/2.],\n",
    "                                 [    0.,    0.,           1.]])\n",
    "\n",
    "    distCoeffsInit = np.zeros((5,1))\n",
    "    #flags = (cv2.CALIB_USE_INTRINSIC_GUESS + cv2.CALIB_RATIONAL_MODEL + cv2.CALIB_FIX_ASPECT_RATIO)\n",
    "    flags = (cv.CALIB_RATIONAL_MODEL)\n",
    "    (ret, camera_matrix, distortion_coefficients0,\n",
    "     rotation_vectors, translation_vectors,\n",
    "     stdDeviationsIntrinsics, stdDeviationsExtrinsics,\n",
    "     perViewErrors) = cv.aruco.calibrateCameraCharucoExtended(\n",
    "                      charucoCorners=allCorners,\n",
    "                      charucoIds=allIds,\n",
    "                      board=board,\n",
    "                      imageSize=imsize,\n",
    "                      cameraMatrix=cameraMatrixInit,\n",
    "                      distCoeffs=distCoeffsInit,\n",
    "                      flags=flags,\n",
    "                      criteria=(cv.TERM_CRITERIA_EPS & cv.TERM_CRITERIA_COUNT, 100000, 1e-9))\n",
    "\n",
    "    return ret, camera_matrix, distortion_coefficients0, rotation_vectors, translation_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that this works for charuco boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retL, mtxL, distL, rvecsL, tvecsL = calibrate_camera(allCorners, allIds, board, imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retR, mtxR, distR, rvecsR, tvecsR = calibrate_camera(allCornersR, allIdsR, board, imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtxL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mtxL for original \n",
    "array([[2.58845450e+03, 0.00000000e+00, 1.62801524e+03],\n",
    "       [0.00000000e+00, 2.58429013e+03, 1.21880158e+03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mtxR\n",
    "array([[2.57613491e+03, 0.00000000e+00, 1.58947057e+03],\n",
    "       [0.00000000e+00, 2.57136577e+03, 1.24136772e+03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distR\n",
    "# this does not look right why is distR is far diffferent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results in lists that can be indexed by imgIndexL\n",
    "ret = [retL, retR]\n",
    "mtx = [mtxL, mtxR]\n",
    "dist = [distL, distR]\n",
    "rvecs = [rvecsL, rvecsR]\n",
    "tvecs = [tvecsL, tvecsR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "camera_calibrationL = {'camera_name':'Left Camera','ret':retL, 'mtx':mtxL, 'dist':distL , 'rvecs':rvecsL, 'tvecs':tvecsL}\n",
    "pickle.dump(camera_calibrationL, open('LeftCameraCalibration.p', 'wb'))\n",
    "\n",
    "camera_calibrationR = {'camera_name':'Right Camera','ret':retR, 'mtx':mtxR, 'dist':distR , 'rvecs':rvecsR, 'tvecs':tvecsR}\n",
    "pickle.dump(camera_calibrationR, open('RightCameraCalibration.p', 'wb'))\n",
    "\n",
    "camera_calibration_combined = {'camera_name':'Combined','ret':ret, 'mtx':mtx, 'dist':dist , 'rvecs':rvecs, 'tvecs':tvecs}\n",
    "pickle.dump(camera_calibration_combined, open('CombinedCameraCalibration.p', 'wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3 # select image id\n",
    "side = imgIndexL\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "frame = cv2.imread(images[i][side])\n",
    "img_undist = cv2.undistort(frame,mtx[side],dist[side],None)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title(\"Raw image\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_undist)\n",
    "plt.title(\"Corrected image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for the big moment\n",
    "\n",
    "From [here](https://vgg.fiit.stuba.sk/2015-02/2783/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereocalib_criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 100000, 1e-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereocalib_flags = (cv2.CALIB_FIX_ASPECT_RATIO \n",
    "        | cv2.CALIB_ZERO_TANGENT_DIST \n",
    "        | cv2.CALIB_SAME_FOCAL_LENGTH \n",
    "        | cv2.CALIB_RATIONAL_MODEL \n",
    "        | cv2.CALIB_FIX_K3 \n",
    "        | cv2.CALIB_FIX_K4 \n",
    "        | cv2.CALIB_FIX_K5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(retval, cameraMatrixL,\n",
    " distCoeffsL, cameraMatrixR, \n",
    " distCoeffsR, R, T, E, F) = cv2.stereoCalibrate(allObjectPoints, allCornersL, allCornersR, \n",
    "                                                mtx[imgIndexL], dist[imgIndexL], \n",
    "                                                mtx[imgIndexR], dist[imgIndexR], imsize,\n",
    "                                                criteria = stereocalib_criteria,\n",
    "                                                flags = stereocalib_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameraMatrixR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx[imgIndexL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectify_flags = cv.CALIB_ZERO_DISPARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(R_L, R_R, \n",
    " P_L, P_R, \n",
    " Q, \n",
    " validPixROIL, validPixROIR) = cv2.stereoRectify(mtx[imgIndexL], dist[imgIndexL], \n",
    "                                                 mtx[imgIndexR], dist[imgIndexR], \n",
    "                                                 imsize_t, R, T, flags=rectify_flags, alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validPixROIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for initUndistortRectifyMap [here](https://docs.rs/opencv/0.22.1/opencv/calib3d/fn.stereo_rectify_camera.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapxL, mapyL = cv2.initUndistortRectifyMap(mtxL, distL, R_L, P_L, imsize_t, cv.CV_32FC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapxR, mapyR = cv2.initUndistortRectifyMap(mtxR, distR, R_R, P_R, imsize_t, cv.CV_32FC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recitfication_maps = {'mapxL':mapxL, 'mapyL':mapyL, 'mapxR':mapxR, 'mapyR':mapyR }\n",
    "pickle.dump(recitfication_maps, open('Rectification_maps.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number=2 # select image id\n",
    "\n",
    "imgL = cv2.imread(images[image_number][0])\n",
    "imgR = cv2.imread(images[image_number][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap the images to rectify them\n",
    "imgL_rect = cv2.remap(imgL, mapxL, mapyL, interpolation=cv.INTER_LINEAR  )\n",
    "imgR_rect = cv2.remap(imgR, mapxR, mapyR, interpolation=cv.INTER_LINEAR  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(18,6))\n",
    "axs[0].imshow(imgL_rect)\n",
    "axs[1].imshow(imgR_rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate((imgL_rect, imgR_rect), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(18,6))\n",
    "axs.imshow(combined)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_horizontal_lines(img, n, color=(255,255,255), thickness=2):\n",
    "    increment = int(img.shape[0]/(n+1))\n",
    "    img_ = img.copy()\n",
    "    \n",
    "    for i in range(1,n+1):\n",
    "        cv2.line(img_, (0, i*increment), (img.shape[1], i*increment), color, thickness, 1)\n",
    "    \n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = draw_horizontal_lines(combined, 30, color=(255,255,255), thickness=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20,10))\n",
    "axs.imshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_charuco_marker_corners(img, board, clipLimit=3.0, verbose=False):\n",
    "\n",
    "    # These are parameters used by the cv2.cornerSubPix function\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.00001)\n",
    "\n",
    "\n",
    "    # increase the contraxt\n",
    "    img = increase_contrast(img, clipLimit=clipLimit)\n",
    " \n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # find the aruco corners and the ids of each corner\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict)\n",
    "\n",
    "    if verbose:\n",
    "        print('Found {} aruco marker corners'.format(len(ids)))\n",
    "        \n",
    "    \n",
    "    #  now find the central points of the aruco markers by averaging the four corner points\n",
    "    if  len(ids) > 0:\n",
    "        # SUB PIXEL DETECTION\n",
    "        for corner in corners:\n",
    "            if verbose:\n",
    "                print('Sub pixel optimization:')\n",
    "                print(corner)\n",
    "            cv2.cornerSubPix(gray, corner,\n",
    "                             winSize = (15,15),\n",
    "                             zeroZone = (-1,-1),\n",
    "                             criteria = criteria)\n",
    "            if verbose:\n",
    "                print(corner)\n",
    "                print('+++')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(corners), ids, gray.shape\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charucoCornersL, charucoIdsL, imsize =  find_charuco_marker_corners(imgL_rect, \n",
    "                                                    board, clipLimit=3.0, \n",
    "                                                    verbose=False, \n",
    "                                                    )\n",
    "charucoCornersR, charucoIdsR, imsize =  find_charuco_marker_corners(imgR_rect, \n",
    "                                                    board, clipLimit=3.0, \n",
    "                                                    verbose=False, \n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corners = np.array(charucoCornersL).reshape((-1,1,2))\n",
    "plot_ids = np.arange(plot_corners.shape[0]).reshape(-1,1)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(36,12))\n",
    "axs.imshow(cv2.aruco.drawDetectedCornersCharuco(imgL_rect, plot_corners, plot_ids))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the common \n",
    "(cornersL_com, \n",
    "  idsL_com, \n",
    "  cornersR_com, \n",
    "      idsR_com) = find_common_corners(charucoCornersL, charucoIdsL, \n",
    "                                                 charucoCornersR, charucoIdsR, \n",
    "                                                 verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corners = np.array(cornersL_com).reshape((-1,1,2))\n",
    "plot_ids = np.arange(plot_corners.shape[0]).reshape(-1,1)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(36,12))\n",
    "axs.imshow(cv2.aruco.drawDetectedCornersCharuco(imgL_rect, plot_corners, plot_ids))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_aruco_center(corners, ids):\n",
    "    # find the center point of the 4 corner of the aruco markers\n",
    "    centers = []\n",
    "    for cnrs in corners:\n",
    "        center = np.array((np.average(cnrs[0][:,0]), np.average(cnrs[0][:,1])))\n",
    "        centers.append(center)\n",
    "    centers = np.array(centers)\n",
    "    centers = np.array(centers).reshape((-1,1,2))\n",
    "    center_ids = np.arange(centers.shape[0]).reshape(-1,1)\n",
    "    \n",
    "    return centers, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centersL, center_idsL = find_aruco_center(cornersL_com, idsL_com)\n",
    "centersR, center_idsR = find_aruco_center(cornersR_com, idsR_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(36,12))\n",
    "axs.imshow(cv2.aruco.drawDetectedCornersCharuco(imgL_rect, centersL, center_idsL))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have the common center in the two pictures \n",
    "\n",
    "# calculate the dispersion between them\n",
    "\n",
    "# calculate the distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = (centersL-centersR)[:,0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera features:\n",
    "- sensor size = 3.68 x 2.76 mm  \n",
    "- sensor resolution  = 3280 × 2464\n",
    "- focal length = 3.04 mm\n",
    "\n",
    "$$ d_{mm} = \\frac{pix \\times 3.68}{3280} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pixels to mm\n",
    "disparity_mm = disparity*3.68/3280\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth can now be found\n",
    "$$ Z = \\frac{T \\times f}{d_{mm}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3.04\n",
    "T = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = f*T/disparity_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = centersL[:,0,0]\n",
    "y = centersL[:,0,1]\n",
    "\n",
    "annot_font = {'fontname':'Arial', 'size':'14','weight':'bold'}\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(36,12))\n",
    "axs.imshow(imgL_rect)\n",
    "axs.scatter(x,y, color='r')\n",
    "for i, txt in enumerate(depth):\n",
    "    axs.annotate('{}'.format(int(txt)), (x[i]+20, y[i]-10), color='r', **annot_font)\n",
    "for i, txt in enumerate(center_idsL.reshape(-1,)):\n",
    "    axs.annotate('{}'.format(int(txt)), (x[i]+20, y[i]+30), color='g',  **annot_font)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each point in each image add the distance to a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_distance = pd.DataFrame(columns=['ArucoId', 'DisparityPix', 'DisparityMM', 'Distance', 'File'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ids in enumerate(center_idsL):\n",
    "    print(i, ids)\n",
    "    ser = pd.Series()\n",
    "    ser['ArucoId']  = int(ids[0])\n",
    "    ser['DisparityPix'] = disparity[i]\n",
    "    ser['DisparityMM'] = disparity_mm[i]\n",
    "    ser['Distance'] = depth[i]\n",
    "    ser['File'] = images[image_number][0]\n",
    "\n",
    "    stereo_distance = stereo_distance.append(ser, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
