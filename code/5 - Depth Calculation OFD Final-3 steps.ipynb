{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Opical Flow analysis\n",
    "\n",
    "By Marc van  Zyl   \n",
    "\n",
    "\n",
    "This notebook uses the OpenCV dense optical flow algorithm by Gunnar Farneback to measure distances to target objects.\n",
    "\n",
    "Optical flow is calculated over the entire image and the results are extracted in the vicinity of the centers of the Aruco target markers (these are found through a separate process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, os\n",
    "from cv2 import aruco\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aruco configuration\n",
    "The following line of the board was updated to reflect the correct scale of the board.  This is necessary becuase the `board` object is used later   \n",
    "`board = aruco.CharucoBoard_create(7, 5, .04026, .8*.04026, aruco_dict)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size = 'experimental setup'\n",
    "if board_size == '7x5':\n",
    "    chessboard_num_squares_across = 7\n",
    "    chessboard_num_squares_up = 5\n",
    "    chessboard_square_size = 0.04026\n",
    "    chessboard_aruco_ratio = 0.8   # this is a fraction of chessboard_square_size\n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)\n",
    "\n",
    "\n",
    "if board_size == '12x8':\n",
    "    chessboard_num_squares_across = 12\n",
    "    chessboard_num_squares_up = 8\n",
    "    chessboard_square_size = 1\n",
    "    chessboard_aruco_ratio = 0.7   # this is a fraction of chessboard_square_size\n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_5X5_250)\n",
    "\n",
    "elif board_size == 'experimental setup':   \n",
    "    aruco_dict = aruco.Dictionary_get(aruco.DICT_4X4_50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the camera calibration matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cam_calOF = pickle.load( open('OFCameraCalibration.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_calOF.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera features:\n",
    "- sensor size = 3.68 x 2.76 mm  \n",
    "- sensor resolution  = 3280 Ã— 2464\n",
    "- focal length = 3.04 mm\n",
    "\n",
    "$$ d_{mm} = \\frac{pix \\times 3.68}{3280} $$\n",
    "\n",
    "The depth can now be found\n",
    "$$ Z = \\frac{T \\times f}{d_{mm}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to increase the contrast in the picture to make the marker identification easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from here https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv\n",
    "def increase_contrast(img, clipLimit=3.0, verbose=False):\n",
    "\n",
    "    if clipLimit>0.0:\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        if verbose:\n",
    "            cv2.imshow(\"lab\",lab)\n",
    "\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        if verbose:\n",
    "            cv2.imshow('l_channel', l)\n",
    "            cv2.imshow('a_channel', a)\n",
    "            cv2.imshow('b_channel', b)\n",
    "\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        if verbose:\n",
    "            cv2.imshow('CLAHE output', cl)\n",
    "\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        if verbose:\n",
    "            cv2.imshow('limg', limg)\n",
    "\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        if verbose:\n",
    "            cv2.imshow('final', final)\n",
    "    else:\n",
    "        final = img.copy()\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find the centers of the Aruco targets\n",
    "This function:\n",
    "1. finds the locations of the corners of the aruco squares (`cv2.aruco.detectMarkers`)\n",
    "1. if markers were found interpolates to find the checkerboard markers between them (`cv2.aruco.interpolateCornersCharuco`)\n",
    "1. zooms into each checkerboard corner to get sub-pixel accuracy using (`cv2.cornerSubPix`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function matches the corners found in the Left and the Right cameras for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_corners(cornersL, idsL, cornersR, idsR, verbose=False):\n",
    "    # find the list of corners common to both images\n",
    "    # get a list of the ids\n",
    "    listIdsL = idsL.reshape((1,-1))[0].tolist()\n",
    "    listIdsR = idsR.reshape((1,-1))[0].tolist()\n",
    "    \n",
    "    # this selects only the points that are in listIdsA and listIdsC\n",
    "    commonIds = list(set(listIdsL).intersection(set(listIdsR)))\n",
    "\n",
    "    if verbose:\n",
    "        print('Found {} common IDS between L:({}) and R:({})'.format(len(commonIds),\n",
    "                                                                     len(idsL),\n",
    "                                                                     len(idsR)))\n",
    "\n",
    "    # Reindex L camera data by taking out only the common points \n",
    "    re_index = [ listIdsL.index(ind) for ind in commonIds]\n",
    "    idsL_new = idsL[re_index]\n",
    "    cornersL_new = np.array(cornersL)[re_index]\n",
    "    cornersL_new = [x for x in cornersL_new]\n",
    "\n",
    "    # Reindex R camera data by taking out only the common points\n",
    "    re_index = [ listIdsR.index(ind) for ind in commonIds]\n",
    "    idsR_new = idsR[re_index]\n",
    "    cornersR_new = np.array(cornersR)[re_index]\n",
    "    cornersR_new = [x for x in cornersR_new]\n",
    "\n",
    "    return np.array(cornersL_new), idsL_new, np.array(cornersR_new), idsR_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_charuco_marker_corners(img, aruco_dict, clipLimit=3.0, verbose=False):\n",
    "\n",
    "    # These are parameters used by the cv2.cornerSubPix function\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10000, 1e-11)\n",
    "\n",
    "    # increase the contraxt\n",
    "    img = increase_contrast(img, clipLimit=clipLimit)\n",
    "\n",
    "\n",
    " \n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    # find the aruco corners and the ids of each corner\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict)\n",
    "\n",
    "    if verbose:\n",
    "        print('Found {} aruco marker corners'.format(len(ids)))\n",
    "        \n",
    "    \n",
    "    #  now find the central points of the aruco markers by averaging the four corner points\n",
    "    if  len(ids) > 0:\n",
    "        # SUB PIXEL DETECTION\n",
    "        for corner in corners:\n",
    "            if verbose:\n",
    "                print('Sub pixel optimization:')\n",
    "                print(corner)\n",
    "            cv2.cornerSubPix(gray, corner,\n",
    "                             winSize = (4,4),\n",
    "                             zeroZone = (-1,-1),\n",
    "                             criteria = criteria)\n",
    "            if verbose:\n",
    "                print(corner)\n",
    "                print('+++')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(corners), ids, gray.shape\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_aruco_center(corners, ids):\n",
    "    # find the center point of the 4 corner of the aruco markers\n",
    "    centers = []\n",
    "    for cnrs in corners:\n",
    "        center = np.array((np.average(cnrs[0][:,0]), np.average(cnrs[0][:,1])))\n",
    "        centers.append(center)\n",
    "    centers = np.array(centers)\n",
    "    centers = np.array(centers).reshape((-1,1,2))\n",
    "    center_ids = np.arange(centers.shape[0]).reshape(-1,1)\n",
    "    \n",
    "    return centers, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/Users/marcvanzyl/Google Drive/ScienceFair/data/Dense Lot/Speed_075/\"\n",
    "\n",
    "\n",
    "video_files = np.array([f for f in os.listdir(datadir) if f.endswith(\".h264\") ])\n",
    "\n",
    "# just sorts the files according to the number so we match picture 1_A with 1_C etc\n",
    "orderR = np.argsort([int((p.split('_')[-1]).split('.')[0]) for p in video_files])\n",
    "video_files = video_files[orderR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set verbose = True to make plots of each frame\n",
    "verbose =  True\n",
    "def analyze_files(video_files):\n",
    "    \n",
    "    for file in video_files:\n",
    "\n",
    "        result_list = []\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        # the size of the square to average the flow over\n",
    "        center_range = 10\n",
    "\n",
    "        cap = cv2.VideoCapture(datadir + file)\n",
    "        print(\"Working with: {}\".format(file))\n",
    "\n",
    "        ret, frame1 = cap.read()\n",
    "        last_pic_3_color = frame1 #cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "        last_pic_3_grey = cv2.cvtColor(last_pic_3_color,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        ret, frame1 = cap.read()\n",
    "        last_pic_2_color = frame1 #cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "        last_pic_2_grey = cv2.cvtColor(last_pic_2_color,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        ret, frame1 = cap.read()\n",
    "        last_pic_1_color = frame1 #cv2.undistort(frame1,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "        last_pic_1_grey = cv2.cvtColor(last_pic_1_color,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        hsv = np.zeros_like(frame1)\n",
    "        hsv[...,1] = 255\n",
    "        flow = np.zeros_like(frame1)\n",
    "\n",
    "        # this creates the array for the sorted centers \n",
    "        sorted_centers = np.zeros([10,2])\n",
    "\n",
    "        while(cap.isOpened()):\n",
    "            ret, curr_pic_color = cap.read()\n",
    "            # ret will be False if there are no more frames\n",
    "            if ret == False:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "            if counter > 120:\n",
    "                break\n",
    "            curr_pic_color = curr_pic_color #cv2.undistort(curr_pic_color,cam_calOF['mtx'],cam_calOF['dist'],None)\n",
    "\n",
    "            curr_pic_grey = cv2.cvtColor(curr_pic_color, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            if counter > 1:\n",
    "                flags = cv2.OPTFLOW_USE_INITIAL_FLOW\n",
    "            else:\n",
    "                flags = 0\n",
    "\n",
    "            flow = cv2.calcOpticalFlowFarneback(last_pic_3_grey, curr_pic_grey, flow, 0.5, 3, 50, 6, 7, 1.2, flags)\n",
    "\n",
    "            # For printing\n",
    "            # mag = simple_norm(np.power(flow[...,0], 3), 3, 25, 0, 255)\n",
    "            #hsv[...,0] = 0\n",
    "            #hsv[...,2] = mag\n",
    "            #rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "            #cv2.imshow('curr_pic_color',rgb)\n",
    "\n",
    "            print('Image Frame Number: {}'.format(counter))\n",
    "\n",
    "\n",
    "            # find the four corners of each aruco marker\n",
    "            arucoCorners, arucoIds, imsize =  find_charuco_marker_corners(curr_pic_color, \n",
    "                                                                aruco_dict, clipLimit=2.0, \n",
    "                                                                verbose=False)\n",
    "\n",
    "            # now find the center point of each aruco marker \n",
    "            centers, center_ids = find_aruco_center(arucoCorners, arucoIds)\n",
    "\n",
    "            # next sort the centers by center_ids from 0 to 9\n",
    "            center_ids = center_ids.reshape(-1)\n",
    "            centers = centers.reshape(-1,2)\n",
    "            print(center_ids)\n",
    "\n",
    "            # insert the centers into the sorted centers array in the right order\n",
    "            for i, ids in enumerate(center_ids):\n",
    "                if (ids < 10):\n",
    "                    sorted_centers[ids] = centers[i]\n",
    "\n",
    "            frame_result_list = []\n",
    "\n",
    "            # for each center extract the 20x20 pixels around it and find the mean\n",
    "            # of the opptical flow in the x and y directions\n",
    "            for center in sorted_centers:\n",
    "                centerx = int(center[1])\n",
    "                centery = int(center[0])\n",
    "                point_result_list = [flow[centerx-center_range:centerx+center_range,\n",
    "                                          centery-center_range:centery+center_range][:,:,0].mean(),\n",
    "                                     flow[centerx-center_range:centerx+center_range,\n",
    "                                          centery-center_range:centery+center_range][:,:,1].mean()]\n",
    "                frame_result_list.append(point_result_list)\n",
    "\n",
    "                print(\"x: {}  Y: {}\".format(point_result_list[0],point_result_list[1]))\n",
    "\n",
    "            result_list.append(frame_result_list)\n",
    "\n",
    "            if verbose:\n",
    "                x = sorted_centers[:,0]\n",
    "                y = sorted_centers[:,1]\n",
    "                mag = (cv2.multiply(cv2.add(flow[...,0],-.20),30))\n",
    "\n",
    "                mag[mag<0.0] = 0\n",
    "                mag[mag>255] = 255\n",
    "\n",
    "                print(\"mean: {} max: {}  min: {} {}\".format(mag.mean(), mag.max(), mag.min(), flow.shape))\n",
    "\n",
    "                # edge detection for the outlines\n",
    "                edges = cv2.Canny(curr_pic_color, 50, 100)\n",
    "\n",
    "\n",
    "                hsv[...,0] = 0\n",
    "                #hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "                hsv[...,2] = mag\n",
    "                rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "\n",
    "                #mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                #hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "                #hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                #rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "                annot_font = {'fontname':'Arial', 'size':'14','weight':'bold'}\n",
    "\n",
    "                fig, axs = plt.subplots(figsize=(36,24))\n",
    "                rgb[edges>100,1] = 255 \n",
    "                axs.imshow(rgb)\n",
    "                axs.scatter(x,y, color='r')\n",
    "                for i in range(10):\n",
    "                    axs.annotate('{}'.format(int(i)), (x[i]+20, y[i]+30), color='r',  **annot_font)\n",
    "\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "            last_pic_3_grey = np.copy(last_pic_2_grey)\n",
    "\n",
    "            last_pic_2_grey = np.copy(last_pic_1_grey)\n",
    "\n",
    "            last_pic_1_grey = np.copy(curr_pic_grey)\n",
    "\n",
    "        pickle.dump(np.array(result_list)[5:105], open('{}{}_res_3.p'.format(datadir, file[:-5]), 'wb'))\n",
    "        result_list = []\n",
    "        cap.release()\n",
    "        cap = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_files(video_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speeds = ['075', '015', '030', '045','060']\n",
    "for speed in speeds:\n",
    "    print(speed)\n",
    "    \n",
    "    datadir = \"/Users/marcvanzyl/Google Drive/ScienceFair/data/Dense Lot/Speed_{}/\".format(speed)\n",
    "    print(datadir)\n",
    "\n",
    "    video_files = np.array([f for f in os.listdir(datadir) if f.endswith(\".h264\") ])\n",
    "    print(video_files)\n",
    "    # just sorts the files according to the number so we match picture 1_A with 1_C etc\n",
    "    orderR = np.argsort([int((p.split('_')[-1]).split('.')[0]) for p in video_files])\n",
    "    video_files = video_files[orderR]\n",
    "    \n",
    "    analyze_files(video_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results for analysis\n",
    "pickle.dump(np.array(result_list), open('{}{}_res.p'.format(datadir, file[:-5]), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{}{}_res.p'.format(datadir, file[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result_list)[5:105].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
